# PenguinCode Configuration
# Target Hardware: NVIDIA RTX 4060 Ti (8GB VRAM)
# All defaults optimized for 8GB VRAM

ollama:
  api_url: "http://localhost:11434"
  timeout: 120

# Global model roles (fallback defaults - optimized for 8GB VRAM)
models:
  planning: "deepseek-coder:6.7b"
  orchestration: "llama3.2:3b"
  research: "llama3.2:3b"
  execution: "qwen2.5-coder:7b"

# Per-agent model overrides (optimized for RTX 4060 Ti 8GB)
agents:
  executor:
    model: "qwen2.5-coder:7b"
    description: "Code mutations, file writes, bash execution"
  explorer:
    model: "llama3.2:3b"
    description: "Codebase navigation, file reading, search"
  reviewer:
    model: "codellama:7b"
    description: "Code review, quality analysis"
  planner:
    model: "deepseek-coder:6.7b"
    description: "Implementation planning, task decomposition"
  tester:
    model: "qwen2.5-coder:7b"
    description: "Test generation and execution"
  refactor:
    model: "codellama:7b"
    description: "Refactoring suggestions and improvements"
  debugger:
    model: "deepseek-coder:6.7b"
    description: "Error analysis, debugging, fix suggestions"
  docs:
    model: "mistral:7b"
    description: "Documentation generation"
  researcher:
    model: "llama3.2:3b"
    description: "Web research, summarization"

defaults:
  temperature: 0.7
  max_tokens: 4096
  context_window: 8192

security:
  level: 2  # 1=always prompt, 2=prompt for destructive, 3=no prompts

history:
  enabled: true
  location: "per-project"
  max_sessions: 50

# Research configuration
research:
  engine: "duckduckgo"  # duckduckgo | fireplexity | sciraai | searxng | google
  use_mcp: true         # Use MCP server when available (default for duckduckgo/google)
  max_results: 5

  # Engine-specific settings
  engines:
    duckduckgo:
      safesearch: "moderate"
      region: "wt-wt"

    fireplexity:
      firecrawl_api_key: "${FIRECRAWL_API_KEY}"
      # Uses Ollama models configured above

    sciraai:
      api_key: "${SCIRA_API_KEY}"
      endpoint: "https://api.scira.ai"

    searxng:
      url: "https://searx.be"  # Public instance or self-hosted
      categories: ["general"]

    google:
      api_key: "${GOOGLE_API_KEY}"
      cx_id: "${GOOGLE_CX_ID}"

# Memory configuration (mem0)
memory:
  enabled: true
  vector_store: "chroma"  # chroma | qdrant | pgvector

  # Uses Ollama models from 'models' section for LLM/embeddings
  embedding_model: "nomic-embed-text"

  stores:
    chroma:
      path: "./.penguincode/memory"
      collection: "penguincode_memory"

    qdrant:
      url: "http://localhost:6333"
      collection: "penguincode_memory"

    pgvector:
      connection_string: "${PGVECTOR_URL}"
      table: "penguincode_memory"

# GPU Regulators (rate limiting to prevent overload)
regulators:
  auto_detect: true
  gpu_type: "auto"
  gpu_model: ""
  vram_mb: 8192
  max_concurrent_requests: 2
  max_models_loaded: 1
  request_queue_size: 10
  min_request_interval_ms: 100
  cooldown_after_error_ms: 1000

# Optional: Hosted Ollama usage API (for quota tracking)
usage_api:
  enabled: false
  endpoint: "https://ollama.example.com/api/usage"
  jwt_token: "${OLLAMA_USAGE_JWT}"
  refresh_interval: 300
  show_warnings_at: 80
